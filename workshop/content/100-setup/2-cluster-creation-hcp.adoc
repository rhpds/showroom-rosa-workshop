= Deploy your cluster using a Hosted Control Plane

In this section we will deploy a ROSA cluster using Hosted Control Plane (HCP).

In short, with ROSA HCP you can decouple the control plane from the data plane (workers).  This is a new deployment model for ROSA in which the control plane is hosted in a Red Hat owned AWS account.  Therefore the control plane is no longer hosted in your AWS account thus reducing your AWS infrastructure expenses. The control plane is dedicated to each cluster and is highly available. See the documentation for more about https://docs.openshift.com/container-platform/4.12/architecture/control-plane.html#hosted-control-planes-overview_control-plane[Hosted Control Planes].

[NOTE]
====
As of this writing Hosted Control Planes (HCP) is currently a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. 
====

== Prerequisites

ROSA HCP requires a few things to be created before deploying the cluster:

. ELB service role
. VPC - This is a "bring-your-own VPC" model (also referred to as BYO-VPC)
. OIDC configuration (and an OIDC provider with that specific configuration)

Let's create those first.

=== Ensure ELB service role exists

Run the following to check for the role and create it if it is missing.

[source,sh,role=execute]
----
aws iam get-role --role-name "AWSServiceRoleForElasticLoadBalancing" || aws iam create-service-linked-role --aws-service-name "elasticloadbalancing.amazonaws.com"
----

=== VPC

. Set a few environment variables for your networking configuration:
+
[source,sh,role=execute]
----
export VPC_CIDR=10.0.0.0/16
export PUBLIC_CIDR_SUBNET=10.0.1.0/24
export PRIVATE_CIDR_SUBNET=10.0.0.0/24
----

. Create the VPC
+
[source,sh,role=execute]
----
export VPC_ID=$(aws ec2 create-vpc --cidr-block $VPC_CIDR --query Vpc.VpcId --output text)
----

. Create Tags:
+
[source,sh,role=execute]
----
aws ec2 create-tags --resources $VPC_ID --tags Key=Name,Value=$CLUSTER_NAME
----

. Enable DNS Hostname:
+
[source,sh,role=execute]
----
aws ec2 modify-vpc-attribute --vpc-id $VPC_ID --enable-dns-hostnames
----

. Now you can create the public subnet and add tags:
+
[source,sh,role=execute]
----
export PUBLIC_SUBNET_ID=$(aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block $PUBLIC_CIDR_SUBNET --query Subnet.SubnetId --output text)

aws ec2 create-tags --resources $PUBLIC_SUBNET_ID --tags Key=Name,Value=$CLUSTER_NAME-public
----

. Create the private subnet and add tags:
+
[source,sh,role=execute]
----
export PRIVATE_SUBNET_ID=$(aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block $PRIVATE_CIDR_SUBNET --query Subnet.SubnetId --output text)

aws ec2 create-tags --resources $PRIVATE_SUBNET_ID --tags Key=Name,Value=$CLUSTER_NAME-private
----

. Create an internet gateway for outbound traffic and attach it to the VPC.
+
[source,sh,role=execute]
----
export IGW_ID=$(aws ec2 create-internet-gateway --query InternetGateway.InternetGatewayId --output text)

aws ec2 create-tags --resources $IGW_ID --tags Key=Name,Value=$CLUSTER_NAME

aws ec2 attach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $IGW_ID
----

. Create a route table for outbound traffic and associate it to the public subnet.
+
[source,sh,role=execute]
----
export PUBLIC_ROUTE_TABLE_ID=$(aws ec2 create-route-table --vpc-id $VPC_ID --query RouteTable.RouteTableId --output text)

aws ec2 create-tags --resources $PUBLIC_ROUTE_TABLE_ID --tags Key=Name,Value=$CLUSTER_NAME

aws ec2 create-route --route-table-id $PUBLIC_ROUTE_TABLE_ID --destination-cidr-block 0.0.0.0/0 --gateway-id $IGW_ID

aws ec2 associate-route-table --subnet-id $PUBLIC_SUBNET_ID --route-table-id $PUBLIC_ROUTE_TABLE_ID
----

. Create a NAT gateway in the public subnet for outgoing traffic from the private network.
+
[source,sh,role=execute]
----
export NAT_IP_ADDRESS=$(aws ec2 allocate-address --domain vpc --query AllocationId --output text)

export NAT_GATEWAY_ID=$(aws ec2 create-nat-gateway --subnet-id $PUBLIC_SUBNET_ID --allocation-id $NAT_IP_ADDRESS --query NatGateway.NatGatewayId --output text)

aws ec2 create-tags --resources $NAT_IP_ADDRESS --resources $NAT_GATEWAY_ID --tags Key=Name,Value=$CLUSTER_NAME
----

. Wait 10 seconds for the NAT gateway to be ready then create a route table for the private subnet to the NAT gateway.
+
[source,sh,role=execute]
----
sleep 10

export PRIVATE_ROUTE_TABLE_ID=$(aws ec2 create-route-table --vpc-id $VPC_ID --query RouteTable.RouteTableId --output text)

aws ec2 create-tags --resources $PRIVATE_ROUTE_TABLE_ID $NAT_IP_ADDRESS --tags Key=Name,Value=$CLUSTER_NAME-private

aws ec2 create-route --route-table-id $PRIVATE_ROUTE_TABLE_ID --destination-cidr-block 0.0.0.0/0 --gateway-id $NAT_GATEWAY_ID

aws ec2 associate-route-table --subnet-id $PRIVATE_SUBNET_ID --route-table-id $PRIVATE_ROUTE_TABLE_ID
----
+
See the https://docs.openshift.com/rosa/rosa_planning/rosa-sts-aws-prereqs.html#rosa-vpc_rosa-sts-aws-prereqs[documentation] for more about VPC requirements.

. Confirm that the environment variables that you need to create the cluster are, in fact, set.
+
[source,sh,role=execute]
----
echo "Public Subnet: $PUBLIC_SUBNET_ID"; echo "Private Subnet: $PRIVATE_SUBNET_ID"
----
+
.Sample Output
[source,text]
----
Public Subnet: subnet-0faeeeb0000000000
Private Subnet: subnet-011fe340000000000
----
+
[WARNING]
====
If one or both are blank, do not proceed and ask for assistance.
====

=== OIDC Configuration

To create the OIDC configuration to be used for your cluster in this workshop, run the following command.  We are opting for the automatic creation mode and Red Hat managed, as this is simpler for the workshop purposes. We are going to store the generated OIDC ID to an environment variable for later use. Notice that the following command uses the ROSA CLI to create your cluster's unique OIDC configuration.

[source,sh,role=execute]
----
export OIDC_ID=$(rosa create oidc-config --mode auto --managed --yes -o json | jq -r '.id'); echo $OIDC_ID;
----

.Sample Output
[source,text]
----
23o3doeo86adgqhci4jl000000000000
----

== Create the cluster
As this is the first time you are deploying ROSA in this account and have not yet created the account roles, create the account-wide roles with policies, and Operator roles with policies.  Since ROSA makes use of AWS Security Token Service (STS), this step creates the AWS IAM roles and policies that are needed for ROSA to interact within your account.  See https://docs.openshift.com/rosa/rosa_architecture/rosa-sts-about-iam-resources.html#rosa-sts-account-wide-roles-and-policies_rosa-sts-about-iam-resources[Account-wide IAM role and policy reference] for more details if you are interested.

. Run the following command to create the account-wide roles:
+
[source,sh,role=execute]
----
rosa create account-roles --mode auto --yes
----

. Run the following command to create the cluster:
+
[source,sh,role=execute]
----
rosa create cluster --cluster-name rosa-${GUID} \
    --subnet-ids ${PUBLIC_SUBNET_ID},${PRIVATE_SUBNET_ID} \
    --hosted-cp \
    --oidc-config-id $OIDC_ID \
    --sts --mode auto --yes
----
+
In about 10 minutes the control plane and API will be up, and about 5-10 minutes after, the worker nodes will be up and the cluster will be completely usable.  This cluster will have a control plane across three AWS availability zones in your selected region, in a Red Hat AWS account and will also create 2 worker nodes in your AWS account.

== Check installation status
. You can run the following command to check the detailed status of the cluster:
+
[source,sh,role=execute]
----
rosa describe cluster --cluster rosa-${GUID}
----
+
or, you can also watch the logs as it progresses:
+
[source,sh,role=execute]
----
rosa logs install --cluster rosa-${GUID} --watch
----

. Once the state changes to “ready” your cluster is now installed. It may take a few more minutes for the worker nodes to come online. In total this should take about 15 minutes.
+
You can continue this lab - there is a step in the next section where you will need to wait for the cluster operators to finish rolling out - but there is no need to wait at this point.
